# -*- coding: utf-8 -*-
"""Al-Quran_Voice Classification_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bjYbP-bgHlb7e-ZYOfBWKi4ZLHyrEet0
"""

!pip install ffmpeg
!pip install librosa
!pip install pydub

import glob
import os
import pandas as pd
from zipfile import ZipFile
import joblib 
import numpy as np
import pydub
import joblib

file_name1 = "/content/drive/MyDrive/Dataset/Al-Quran App Data/AL Quran App (Voice Recognition)/Total Data MP3.zip"

with ZipFile(file_name1, 'r') as zip:
  zip.extractall()
  print('Done')

file_name2 = "/content/drive/MyDrive/Dataset/Al-Quran App Data/AL Quran App (Voice Recognition)/Quran App Data (2nd Part).zip"

with ZipFile(file_name2, 'r') as zip:
  zip.extractall()
  print('Done')

#/content/Quran App
audioFolder = glob.glob("/content/Quran App/*.zip")
audioFolder



for i in range(358):
  FileName = audioFolder[i]
  with ZipFile(FileName, 'r') as zip:
    zip.extractall()
    print('Done')



#/content/Quran App
audio = glob.glob("/content/*.zip")
#audio
for i in audio:
  FileName = i
  with ZipFile(FileName, 'r') as zip:
    zip.extractall()
    print('Done')

audio = glob.glob("/content/*.mp3")

len(audio)

cd /content/drive/MyDrive/Dataset/Al-Quran App Data/AL Quran App (Voice Recognition)

!pwd

# create a ZipFile object
zipObj = ZipFile('mp3.zip', 'w')
for k in audio:

  # Add multiple files to the zip
  zipObj.write(k)
  
# close the Zip File
zipObj.close()

# Now With Total Datasets
#/content/drive/MyDrive/Dataset/Al-Quran App Data/AL Quran App (Voice Recognition)/Total Data MP3.zip

cd /content

import os 
    
# Directory 
directory = "Dataset"
    
# Parent Directory path 
parent_dir = "/content/"
    
# Path 
path = os.path.join(parent_dir, directory) 
    
# Create the directory 
# 'GeeksForGeeks' in 
# '/home / User / Documents' 
os.mkdir(path) 
print("Directory '% s' created" % directory)

cd /content/

file_Total = "/content/drive/MyDrive/Dataset/Al-Quran App Data/AL Quran App (Voice Recognition)/Total Data MP3.zip"

with ZipFile(file_Total, 'r') as zip:
  zip.extractall()
  print('Done')

#Total MP3 files 
#/content/Quran App
#/content/drive/MyDrive/Dataset/Al-Quran App Data/AL Quran App (Voice Recognition)/datasetflac.pkl
audioFolder = glob.glob("content/*.flac")
audioFolder

cd /content/drive/MyDrive/Dataset/Al-Quran App Data/AL Quran App (Voice Recognition)/flac dataset

# create a ZipFile object
zipObj = ZipFile('flac_format_Dataset.zip', 'w')
for k in audioFolder:

  # Add multiple files to the zip
  zipObj.write("/content/" + k)
  
# close the Zip File
zipObj.close()
#/content/content/Nuruddin-mohamed-el-ouakili-001-al-fatiha-46354-3100.flac

#Converting MP3 to flac format
from os import path
from pydub import AudioSegment

# files 
for f in audioFolder:                                                                        
  src = f
  dst = str(f[:-3])+"flac"

# convert wav to mp3                                                            
  sound = AudioSegment.from_mp3(src)
  sound.export(dst, format="flac")

#list the files
#filelist = os.listdir('/content') 
filelist = audioFolder
#read them into pandas
train_df = pd.DataFrame(filelist)

# Renaming the column name to file
train_df = train_df.rename(columns={0:'file'})

# Code in case we have to drop the '.DS_Store' and reset the index
train_df[train_df['file']=='.DS_Store']
train_df.drop(16, inplace=True)
train_df = train_df.sample(frac=1).reset_index(drop=True)

# We create an empty list where we will append all the speakers ids for each row of our dataframe by slicing the file name since we know the id is the first number before the hash
speaker = []
speaker2 = []

for i in range(0, len(train_df)):
    speaker2.append(train_df['file'][i].split('-')[0:5])
    #train_df['file'][i].split('-')[0:5]
    temp = train_df['file'][i].split('-')[0:5]
    sep = ''
    for c in temp:
      sep = sep + str(c)  + " "
    speaker.append(sep)
# We now assign the speaker to a new column 
train_df['speaker'] = speaker
train_df['speaker2'] = speaker2 #Correct Name with out Dashes

train_df.head()

name = []
for var in train_df['speaker']:
  str(var)
  if var[:var.find("0")]:
    name.append(var[:var.find("0")])
  elif var[:string.find("1")]:
    name.append(var[:var.find("1")])
  
train_df['Name'] = name
train_df.head()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
#print(le.fit_transform(["paris", "my v 55 ", "tokyo japan", " 22 amsterdam"]))
#print(le.fit_transform(train_df['Name']))
train_df['speake'] = le.fit_transform(train_df['Name'])
train_df.head()

Data = train_df[['file', 'speake']]
Data.head()

cd /content/drive/MyDrive/Dataset/Al-Quran App Data/AL Quran App (Voice Recognition)

#cd /content/drive/MyDrive/Dataset/Al-Quran App Data/AL Quran App (Voice Recognition)
# Save the model as a pickle in a file 
joblib.dump(Data, 'datasetflac.pkl')

## Getting Preprocessed Data From Drive

Data = joblib.load('/content/drive/MyDrive/Dataset/Al-Quran App Data/AL Quran App (Voice Recognition)/datasetflac.pkl')

Data.head()
#Data.info(memory_usage="deep")



import librosa
def extract_features(files):
    
    # Sets the name to be the path to where the file is in my computer
    file_name = os.path.join(os.path.abspath('/content')+'/'+str(files.file))
# Loads the audio file as a floating point time series and assigns the default sample rate
    # Sample rate is set to 22050 by default
    X, sample_rate = librosa.load(file_name, res_type='kaiser_fast')
# Generate Mel-frequency cepstral coefficients (MFCCs) from a time series 
    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)
# Generates a Short-time Fourier transform (STFT) to use in the chroma_stft
    stft = np.abs(librosa.stft(X))
# Computes a chromagram from a waveform or power spectrogram.
    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)
# Computes a mel-scaled spectrogram.
    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)
# Computes spectral contrast
    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)
# Computes the tonal centroid features (tonnetz)
    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X),
    sr=sample_rate).T,axis=0)
    return mfccs, chroma, mel, contrast, tonnetz

train_features = Data.apply(extract_features, axis=1)

train_features

cd /content/drive/MyDrive/Dataset/Al-Quran App Data/AL Quran App (Voice Recognition)
# Save the model as a pickle in a file 
joblib.dump(train_features, 'train_features.pkl')

